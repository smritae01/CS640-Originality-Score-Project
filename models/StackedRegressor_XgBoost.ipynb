{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic here - \n",
    "# read the data and then viz it for the first 100 samples\n",
    "# do not apply anything from bag of tricks first, just see if it works\n",
    "# plot the results accordingly\n",
    "# revisit after first 100 samples train for the 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eba5b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e70ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Open AI Classfier  Zero GPT  GPT Zero  Label          ID\n",
      "0                0.0       1.0       1.0    0.0  63,064,638\n",
      "1                0.0       1.0       1.0    0.0     279,621\n",
      "2                0.0       0.0       1.0    0.0     287,229\n",
      "3                0.0       0.0       0.0    0.0    26712375\n",
      "4                0.0       0.0       0.0    0.0    38894426\n",
      "5                0.0       0.0       1.0    0.0    26709147\n",
      "6                0.0       0.0       1.0    0.0    15169469\n",
      "7                0.0       0.0       0.0    0.0     5723289\n",
      "8                0.0       0.0       0.0    0.0    62862417\n",
      "9                0.0       0.0       1.0    0.0    62166018\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Read the predictions and ground truth from the CSV file\n",
    "data = pd.read_csv('/home/ssj/CS640-Originality-Score-Project/data/models-output.csv')\n",
    "print(data.head(10))\n",
    "data.dropna(inplace=True)\n",
    "# y_pred = data.iloc[:, :3].values\n",
    "\n",
    "# print(y_pred)\n",
    "y_true = data.iloc[:, 3].values\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9178ff22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the predictions and ground truth from the CSV file\n",
    "data = pd.read_csv('/home/ssj/CS640-Originality-Score-Project/data/AI_data.csv')\n",
    "\n",
    "# drop columns\n",
    "data = data.drop(['Unnamed: 8', 'DetectGPT', 'GPT Zero '], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 1 is AI / 0 is Human\n",
    "data['Label'] = 1 # 1 is the highest probability, which means def AI \n",
    "\n",
    "# viz the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col = data.columns\n",
    "# print(col)\n",
    "\n",
    "# create predictions to train the meta regressor\n",
    "y_pred = data.iloc[:, 2:6].values\n",
    "\n",
    "# create ground truth labels\n",
    "y_true = data['Label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9293b9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.000000e+00 2.618200e+01 1.461400e+01 7.820000e-01]\n",
      " [4.000000e+00 4.580000e+01 4.440900e+01 1.378000e+00]\n",
      " [4.000000e+00 6.953800e+01 9.929000e+01 1.533000e+00]\n",
      " [4.000000e+00 2.633300e+01 1.742700e+01 1.396000e+00]\n",
      " [4.000000e+00 2.810830e+02 8.526870e+02 1.506000e+00]\n",
      " [4.000000e+00 1.399091e+03 4.572934e+03 1.171000e+00]\n",
      " [4.000000e+00 4.415400e+01 3.923200e+01 1.510000e+00]\n",
      " [4.000000e+00 5.578600e+01 6.496100e+01 1.091000e+00]\n",
      " [4.000000e+00 6.546200e+01 1.378700e+02 1.365000e+00]\n",
      " [4.000000e+00 4.683300e+01 5.467600e+01 1.094000e+00]\n",
      " [4.000000e+00 3.521400e+01 7.677500e+01 1.196000e+00]\n",
      " [4.000000e+00 3.290900e+01 2.503800e+01 2.107000e+00]\n",
      " [4.000000e+00 6.541700e+01 5.325500e+01 1.580000e-01]\n",
      " [4.000000e+00 5.670000e+01 1.015740e+02 1.207000e+00]\n",
      " [4.000000e+00 1.462310e+02 3.848950e+02 8.970000e-01]\n",
      " [4.000000e+00 6.629400e+01 8.812900e+01 1.286000e+00]\n",
      " [4.000000e+00 1.936400e+01 1.433400e+01 1.252000e+00]\n",
      " [4.000000e+00 2.570000e+01 1.972100e+01 1.079000e+00]\n",
      " [4.000000e+00 2.454500e+01 1.173300e+01 1.598000e+00]\n",
      " [2.000000e+00 4.363600e+01 3.012400e+01 1.208000e+00]\n",
      " [4.000000e+00 5.900000e+01 8.842800e+01 9.830000e-01]\n",
      " [4.000000e+00 4.978600e+01 9.472100e+01 1.431000e+00]\n",
      " [4.000000e+00 4.182140e+02 1.209476e+03 1.133000e+00]\n",
      " [4.000000e+00 9.409100e+01 2.282680e+02 7.800000e-01]\n",
      " [4.000000e+00 6.269200e+01 1.232090e+02 1.097000e+00]\n",
      " [4.000000e+00 4.207700e+01 6.521800e+01 1.050000e+00]\n",
      " [4.000000e+00 4.384600e+01 1.965000e+01 1.907000e+00]\n",
      " [3.000000e+00 3.711100e+01 3.074300e+01 1.195000e+00]\n",
      " [3.000000e+00 7.080000e+01 1.618550e+02 1.407000e+00]\n",
      " [3.000000e+00 1.841700e+01 9.624000e+00 1.022000e+00]\n",
      " [4.000000e+00 2.361500e+01 1.326400e+01 1.574000e+00]\n",
      " [4.000000e+00 2.425000e+01 3.138700e+01 1.016000e+00]\n",
      " [4.000000e+00 2.215400e+01 2.035900e+01 1.324000e+00]\n",
      " [3.000000e+00 2.566700e+01 1.258700e+01 9.650000e-01]\n",
      " [4.000000e+00 3.209100e+01 3.524000e+01 1.115000e+00]\n",
      " [4.000000e+00 2.075000e+01 8.946000e+00 2.112000e+00]\n",
      " [4.000000e+00 4.646200e+01 3.306700e+01 1.098000e+00]\n",
      " [4.000000e+00 4.515400e+01 3.340100e+01 2.115000e+00]\n",
      " [4.000000e+00 3.707100e+01 3.555200e+01 2.340000e-01]\n",
      " [3.000000e+00 5.638500e+01 5.360000e+01 2.397000e+00]\n",
      " [4.000000e+00 3.875000e+01 1.427100e+01 8.700000e-01]\n",
      " [4.000000e+00 3.300000e+01 3.969300e+01 1.006000e+00]\n",
      " [4.000000e+00 5.021400e+01 5.830600e+01 8.740000e-01]\n",
      " [4.000000e+00 4.176900e+01 2.099300e+01 1.114000e+00]\n",
      " [4.000000e+00 2.405000e+02 7.610720e+02 1.154000e+00]\n",
      " [4.000000e+00 8.021400e+01 1.680510e+02 1.656000e+00]\n",
      " [4.000000e+00 4.933300e+01 4.297000e+01 1.190000e+00]\n",
      " [4.000000e+00 3.415400e+01 1.564600e+01 1.022000e+00]\n",
      " [4.000000e+00 3.718200e+01 4.163800e+01 1.228000e+00]\n",
      " [4.000000e+00 3.715400e+01 2.422400e+01 3.390000e+00]\n",
      " [4.000000e+00 3.853300e+01 1.856200e+01 1.292000e+00]\n",
      " [4.000000e+00 3.460000e+01 4.124800e+01 1.150000e+00]\n",
      " [3.000000e+00 8.129090e+02 2.636758e+03 1.805000e+00]\n",
      " [4.000000e+00 3.020000e+01 2.099400e+01 1.715000e+00]\n",
      " [4.000000e+00 8.571400e+01 2.343420e+02 8.780000e-01]\n",
      " [3.000000e+00 2.358300e+01 9.100000e+00 1.640000e+00]\n",
      " [4.000000e+00 3.100000e+01 1.739500e+01 1.256000e+00]\n",
      " [4.000000e+00 1.151820e+02 3.025440e+02 1.309000e+00]\n",
      " [4.000000e+00 4.768800e+01 2.140600e+01 9.630000e-01]\n",
      " [4.000000e+00 2.712500e+01 1.458300e+01 1.743000e+00]\n",
      " [4.000000e+00 3.291700e+01 1.707400e+01 1.240000e+00]\n",
      " [4.000000e+00 2.983300e+01 1.954900e+01 9.880000e-01]\n",
      " [4.000000e+00 4.269200e+01 5.575700e+01 1.355000e+00]\n",
      " [3.000000e+00 6.236400e+01 1.047470e+02 1.623000e+00]\n",
      " [3.000000e+00 6.784600e+01 9.838200e+01 1.787000e+00]\n",
      " [4.000000e+00 8.426700e+01 1.468710e+02 1.304000e+00]\n",
      " [2.000000e+00 1.057690e+02 2.803330e+02 1.479000e+00]\n",
      " [4.000000e+00 1.474000e+02 4.420510e+02 1.596000e+00]\n",
      " [3.000000e+00 4.050000e+01 5.714500e+01 1.527000e+00]\n",
      " [4.000000e+00 8.553300e+01 2.163600e+02 1.286000e+00]\n",
      " [4.000000e+00 2.112500e+01 9.749000e+00 1.614000e+00]\n",
      " [4.000000e+00 8.058300e+01 1.058550e+02 1.147000e+00]\n",
      " [4.000000e+00 2.972700e+01 2.642000e+01 1.509000e+00]\n",
      " [4.000000e+00 6.223100e+01 5.284100e+01 6.820000e-01]\n",
      " [4.000000e+00 4.440000e+01 5.638200e+01 1.042000e+00]\n",
      " [4.000000e+00 3.307100e+01 4.318000e+01 2.175000e+00]\n",
      " [4.000000e+00 4.075000e+01 5.259600e+01 1.505000e+00]\n",
      " [4.000000e+00 3.462500e+01 2.934100e+01 1.148000e+00]\n",
      " [4.000000e+00 4.866700e+01 9.384100e+01 1.399000e+00]\n",
      " [3.000000e+00 5.638500e+01 6.007000e+01 5.120000e-01]\n",
      " [4.000000e+00 2.790000e+01 1.737500e+01 1.350000e+00]\n",
      " [4.000000e+00 4.775000e+01 5.398800e+01 1.385000e+00]\n",
      " [3.000000e+00 3.192900e+01 4.680100e+01 1.495000e+00]\n",
      " [4.000000e+00 6.935700e+01 1.257880e+02 7.550000e-01]\n",
      " [3.000000e+00 2.645500e+01 1.559100e+01 1.340000e+00]\n",
      " [3.000000e+00 2.446200e+01 1.537100e+01 2.034000e+00]\n",
      " [4.000000e+00 4.414300e+01 2.598500e+01 1.551000e+00]\n",
      " [4.000000e+00 2.481800e+01 8.376000e+00 1.942000e+00]\n",
      " [4.000000e+00 4.790900e+01 3.894000e+01 8.150000e-01]\n",
      " [4.000000e+00 3.684600e+01 2.864700e+01 1.210000e+00]\n",
      " [4.000000e+00 2.946700e+01 2.382000e+01 1.599000e+00]\n",
      " [4.000000e+00 3.554500e+01 1.882200e+01 1.063000e+00]\n",
      " [2.000000e+00 3.981800e+01 4.790400e+01 1.816000e+00]\n",
      " [4.000000e+00 6.446700e+01 1.037450e+02 1.117000e+00]\n",
      " [4.000000e+00 6.720000e+01 1.698660e+02 1.635000e+00]\n",
      " [2.000000e+00 4.492900e+01 3.640500e+01 1.197000e+00]\n",
      " [2.000000e+00 3.108300e+01 1.790100e+01 1.698000e+00]\n",
      " [1.000000e+00 1.900000e+01 1.143100e+01 1.473000e+00]\n",
      " [4.000000e+00 6.441700e+01 9.047100e+01 1.349000e+00]\n",
      " [4.000000e+00 1.029170e+02 2.617850e+02 4.450000e-01]\n",
      " [4.000000e+00 8.300000e+01 2.121260e+02 9.030000e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2440d891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d391b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read the predictions and ground truth from the CSV file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# data = pd.read_csv('../data/models-output.csv')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# data.dropna(inplace=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Standardize the input features\u001b[39;00m\n\u001b[1;32m     13\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m---> 14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43my_pred\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define an MLPRegressor to learn the weights of the models\u001b[39;00m\n\u001b[1;32m     17\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLPRegressor(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the predictions and ground truth from the CSV file\n",
    "# data = pd.read_csv('../data/models-output.csv')\n",
    "# data.dropna(inplace=True)\n",
    "# y_pred = data.iloc[:, :3].values\n",
    "# y_true = data.iloc[:, 3].values\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "y_pred = scaler.fit_transform(y_pred)\n",
    "\n",
    "# Define an MLPRegressor to learn the weights of the models\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(1,), activation='relu', solver='adam', max_iter=1000)\n",
    "\n",
    "# Fit the MLPRegressor on the predictions and ground truth\n",
    "mlp.fit(y_pred, y_true)\n",
    "\n",
    "# Get the learned weights from the MLPRegressor\n",
    "weights = mlp.coefs_[0]\n",
    "\n",
    "# Normalize the weights to sum up to 1\n",
    "weights /= np.sum(weights)\n",
    "\n",
    "print(\"Weights obtained:\\n\")\n",
    "print(\"Open AI Classifier:\", weights[0][0])\n",
    "print(\"Zero GPT:\", weights[1][0])\n",
    "print(\"GPT Zero:\", weights[2][0])\n",
    "print(\"\\n\")\n",
    "y_combined = np.dot(y_pred, weights)\n",
    "print(\"Predicted labels:\\n\", y_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50d4af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d0d93a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid 'estimators' attribute, 'estimators' should be a non-empty list of (string, estimator) tuples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m stacked_regressor \u001b[38;5;241m=\u001b[39m StackingRegressor(\n\u001b[1;32m     13\u001b[0m     estimators\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     14\u001b[0m     final_estimator\u001b[38;5;241m=\u001b[39mXGBRegressor()\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# fit the stacked regressor on your data\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mstacked_regressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# make predictions with the stacked regressor\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# y_pred = stacked_regressor.predict(X_test)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs640/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:958\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the estimators.\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124;03m    Returns a fitted instance.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    957\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cs640/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py:194\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# all_estimators contains all estimators, the one to be fitted and the\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# 'drop' string.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m names, all_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_estimators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_final_estimator()\n\u001b[1;32m    197\u001b[0m stack_method \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_estimators)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs640/lib/python3.9/site-packages/sklearn/ensemble/_base.py:278\u001b[0m, in \u001b[0;36m_BaseHeterogeneousEnsemble._validate_estimators\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_estimators\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimators\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m attribute, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimators\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-empty list of (string, estimator) tuples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         )\n\u001b[1;32m    282\u001b[0m     names, estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# defined by MetaEstimatorMixin\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid 'estimators' attribute, 'estimators' should be a non-empty list of (string, estimator) tuples."
     ]
    }
   ],
   "source": [
    "# # load your original feature data into X and your target variable into y\n",
    "# X = pd.read_csv('original_features.csv')\n",
    "# y = pd.read_csv('target_variable.csv')\n",
    "\n",
    "# # load the CSV file containing the outputs of your pretrained models\n",
    "# pretrained_outputs = pd.read_csv('pretrained_outputs.csv')\n",
    "\n",
    "# # concatenate the pretrained outputs with your original feature data\n",
    "# X = pd.concat([X, pretrained_outputs], axis=1)\n",
    "\n",
    "# create the stacked regressor with XGBoost as the final estimator\n",
    "stacked_regressor = StackingRegressor(\n",
    "    estimators=[],\n",
    "    final_estimator=XGBRegressor()\n",
    ")\n",
    "\n",
    "# fit the stacked regressor on your data\n",
    "stacked_regressor.fit(y_pred, y_true)\n",
    "\n",
    "# make predictions with the stacked regressor\n",
    "# y_pred = stacked_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38709149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A Multilayer perceptron to compute weights of each of the AI detection models according to the outputs they predicted\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "frpm\n",
    "\n",
    "# Read the predictions and ground truth from the CSV file\n",
    "data = pd.read_csv('../data/models-output.csv')\n",
    "data.dropna(inplace=True)\n",
    "y_pred = data.iloc[:, :3].values\n",
    "\n",
    "print(y_pred)\n",
    "y_true = data.iloc[:, 3].values\n",
    "print(y_true)\n",
    "\n",
    "# Define an MLPRegressor to learn the weights of the models\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(1,), max_iter=1000)\n",
    "\n",
    "# Fit the MLPRegressor on the predictions and ground truth\n",
    "mlp.fit(y_pred, y_true)\n",
    "\n",
    "# Get the learned weights from the MLPRegressor\n",
    "weights = mlp.coefs_[0]\n",
    "\n",
    "# Normalize the weights to sum up to 1\n",
    "weights /= np.sum(weights)\n",
    "print(\"Weights obtained:\\n\")\n",
    "print(\"Open AI Classifier:\", weights[0][0])\n",
    "print(\"Zero GPT:\", weights[1][0])\n",
    "print(\"GPT Zero:\", weights[2][0])\n",
    "print(\"\\n\")\n",
    "y_combined = np.dot(y_pred, weights)\n",
    "print(\"Predicted labels:\\n\", y_combined)\n",
    "# print(len(y_combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cd77c7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights obtained:\n",
      "\n",
      "Open AI Classifier: 0.2538721669893282\n",
      "Zero GPT: -0.03303723171850475\n",
      "GPT Zero: -0.336352819182487\n",
      "\n",
      "\n",
      "Predicted labels:\n",
      " [[ 0.6753484   0.38014216  0.00606358 ...  0.04408033  0.44094343\n",
      "   0.38341928]\n",
      " [ 0.14645544  0.20159932 -0.16764337 ...  0.30691699  0.18962176\n",
      "   0.13023946]\n",
      " [-0.02453126  0.07005697 -0.2009443  ...  0.3443073   0.1057537\n",
      "   0.03402207]\n",
      " ...\n",
      " [ 0.13832059  0.12282715 -0.14582533 ...  0.2626728   0.18190036\n",
      "   0.1099813 ]\n",
      " [ 0.79049215  0.0394602   0.14136914 ... -0.27290917  0.50665476\n",
      "   0.39774645]\n",
      " [ 0.4368316   0.04663906 -0.0215983  ... -0.02132938  0.35239732\n",
      "   0.26566929]]\n",
      "sum of weights: [-0.49225567 -0.878974   -0.42836428 -0.09255096  0.114144    0.07123152\n",
      " -0.34193488  0.32773551  0.13269562 -0.12160887]\n"
     ]
    }
   ],
   "source": [
    "# new mlp with the final code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the predictions and ground truth from the CSV file\n",
    "data = pd.read_csv('/home/ssj/CS640-Originality-Score-Project/data/AI_data.csv')\n",
    "\n",
    "# drop columns\n",
    "data = data.drop(['Unnamed: 8', 'DetectGPT', 'GPT Zero '], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 1 is AI / 0 is Human\n",
    "data['Label'] = 1 # 1 is the highest probability, which means def AI \n",
    "\n",
    "# viz the data\n",
    "# data.head(10)\n",
    "\n",
    "# col = data.columns\n",
    "# print(col)\n",
    "\n",
    "# create predictions to train the MLP regressor\n",
    "y_pred = data.iloc[:, 2:6].values\n",
    "\n",
    "# create ground truth labels\n",
    "y_true = data['Label'].values\n",
    "\n",
    "# Read the predictions and ground truth from the CSV file\n",
    "# data = pd.read_csv('../data/models-output.csv')\n",
    "# data.dropna(inplace=True)\n",
    "# y_pred = data.iloc[:, :3].values\n",
    "# y_true = data.iloc[:, 3].values\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "y_pred = scaler.fit_transform(y_pred)\n",
    "\n",
    "# Define an MLPRegressor to learn the weights of the models\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10,7,3), activation='relu', solver='adam', max_iter=10000)\n",
    "\n",
    "# Fit the MLPRegressor on the predictions and ground truth\n",
    "mlp.fit(y_pred, y_true)\n",
    "\n",
    "# Get the learned weights from the MLPRegressor\n",
    "weights = mlp.coefs_[0]\n",
    "\n",
    "# Calculate the sum of the absolute values of the weights\n",
    "abs_weights_sum = np.sum(np.abs(weights))\n",
    "\n",
    "# Normalize the weights to sum up to 1\n",
    "normalized_weights = weights / abs_weights_sum\n",
    "\n",
    "print(\"Weights obtained:\\n\")\n",
    "print(\"Open AI Classifier:\", normalized_weights[0][0])\n",
    "print(\"Zero GPT:\", normalized_weights[1][0])\n",
    "print(\"GPT Zero:\", normalized_weights[2][0])\n",
    "print(\"\\n\")\n",
    "y_combined = np.dot(y_pred, normalized_weights)\n",
    "print(\"Predicted labels:\\n\", y_combined)\n",
    "print(\"sum of weights:\", np.abs(normalized_weights[0][0])+np.abs(normalized_weights[1][0])+np.abs(normalized_weights[2][0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get the learned weights from the MLPRegressor\n",
    "# weights = mlp.coefs_[0]\n",
    "\n",
    "# # Calculate the sum of the absolute values of the weights along the rows\n",
    "# abs_weights_sum = np.sum(np.abs(weights), axis=1)\n",
    "\n",
    "# # Normalize the weights row-wise to sum up to 1\n",
    "# normalized_weights = weights / abs_weights_sum[:, np.newaxis]\n",
    "\n",
    "# print(\"Weights obtained:\\n\")\n",
    "# print(\"Open AI Classifier:\", normalized_weights[0][0])\n",
    "# print(\"Zero GPT:\", normalized_weights[1][0])\n",
    "# print(\"GPT Zero:\", normalized_weights[2][0])\n",
    "# print(\"\\n\")\n",
    "# y_combined = np.dot(y_pred, normalized_weights)\n",
    "# print(\"Predicted labels:\\n\", y_combined)\n",
    "# print(\"sum of weights:\", np.sum(normalized_weights, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "# Get the learned weights from the MLPRegressor\n",
    "weights = mlp.coefs_[0]\n",
    "\n",
    "# Calculate the sum of the absolute values of the weights along the columns\n",
    "abs_weights_sum = np.sum(np.abs(weights), axis=0)\n",
    "\n",
    "# Normalize the weights column-wise to sum up to 1\n",
    "normalized_weights = weights / abs_weights_sum\n",
    "\n",
    "print(\"Weights obtained:\\n\")\n",
    "print(\"Open AI Classifier:\", normalized_weights[0][0])\n",
    "print(\"Zero GPT:\", normalized_weights[1][0])\n",
    "print(\"GPT Zero:\", normalized_weights[2][0])\n",
    "print(\"\\n\")\n",
    "y_combined = np.dot(y_pred, normalized_weights)\n",
    "print(\"Predicted labels:\\n\", y_combined)\n",
    "print(\"sum of weights:\", np.sum(normalized_weights, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad2ec263",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'MLPRegressor(hidden_layer_sizes=(10, 7, 3, 1), max_iter=10000)' (type <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 39\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Create a pipeline to apply the custom activation function\u001b[39;00m\n\u001b[1;32m     34\u001b[0m mlp_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     35\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m'\u001b[39m, mlp),\n\u001b[1;32m     36\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m, SoftmaxTransformer())\n\u001b[1;32m     37\u001b[0m ])\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmlp_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m weights \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mcoefs_[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Access the last layer weights\u001b[39;00m\n\u001b[1;32m     43\u001b[0m abs_weights_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(weights))\n",
      "File \u001b[0;32m~/anaconda3/envs/cs640/lib/python3.9/site-packages/sklearn/pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cs640/lib/python3.9/site-packages/sklearn/pipeline.py:339\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps):\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps)\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m# Setup the memory\u001b[39;00m\n\u001b[1;32m    341\u001b[0m     memory \u001b[38;5;241m=\u001b[39m check_memory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory)\n",
      "File \u001b[0;32m~/anaconda3/envs/cs640/lib/python3.9/site-packages/sklearn/pipeline.py:230\u001b[0m, in \u001b[0;36mPipeline._validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    228\u001b[0m         t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m     ):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll intermediate steps should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers and implement fit and transform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor be the string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (t, \u001b[38;5;28mtype\u001b[39m(t))\n\u001b[1;32m    235\u001b[0m         )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# We allow last estimator to be None as an identity transformation\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    239\u001b[0m     estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m ):\n",
      "\u001b[0;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'MLPRegressor(hidden_layer_sizes=(10, 7, 3, 1), max_iter=10000)' (type <class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>) doesn't"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Custom activation function to normalize the output\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "class SoftmaxTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.apply_along_axis(softmax, 1, X)\n",
    "\n",
    "data = pd.read_csv('/home/ssj/CS640-Originality-Score-Project/data/AI_data.csv')\n",
    "data = data.drop(['Unnamed: 8', 'DetectGPT', 'GPT Zero '], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "data['Label'] = 1\n",
    "y_pred = data.iloc[:, 2:6].values\n",
    "y_true = data['Label'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y_pred = scaler.fit_transform(y_pred)\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10,7,3,1), activation='relu', solver='adam', max_iter=10000)\n",
    "\n",
    "# Create a pipeline to apply the custom activation function\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('mlp', mlp),\n",
    "    ('softmax', SoftmaxTransformer())\n",
    "])\n",
    "\n",
    "mlp_pipeline.fit(y_pred, y_true)\n",
    "\n",
    "weights = mlp.coefs_[-1]  # Access the last layer weights\n",
    "\n",
    "abs_weights_sum = np.sum(np.abs(weights))\n",
    "\n",
    "normalized_weights = weights / abs_weights_sum\n",
    "\n",
    "print(\"Weights obtained:\\n\")\n",
    "print(\"Open AI Classifier:\", normalized_weights[0][0])\n",
    "print(\"Zero GPT:\", normalized_weights[1][0])\n",
    "print(\"GPT Zero:\", normalized_weights[2][0])\n",
    "print(\"\\n\")\n",
    "y_combined = np.dot(y_pred, normalized_weights)\n",
    "print(\"Predicted labels:\\n\", y_combined)\n",
    "print(\"sum of weights:\", np.abs(normalized_weights[0][0])+np.abs(normalized_weights[1][0])+np.abs(normalized_weights[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8bfd76e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights obtained:\n",
      "\n",
      "Open AI Classifier: 0.19331599597683388\n",
      "GPT Zero:(perplexity) 0.10254274536930445\n",
      "GPT Zero:(burstiness) -0.29176235559384267\n",
      "DetectGPT:(Z score) 0.412378903060019\n",
      "\n",
      "\n",
      "sum of weights:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# TODO implement better activation function with custom MLP regressor class with embedded forward pass\n",
    "# TODO update model with the updated dataset columns\n",
    "\n",
    "# Custom MLPRegressor with transform method\n",
    "class CustomMLPRegressor(MLPRegressor):\n",
    "    def transform(self, X):\n",
    "        return softmax(self.predict(X))\n",
    "\n",
    "# Custom activation function to normalize the output\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "data = pd.read_csv('/home/ssj/CS640-Originality-Score-Project/data/AI_data.csv')\n",
    "data = data.drop(['Unnamed: 8', 'DetectGPT', 'GPT Zero '], axis=1)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# data.head(10)\n",
    "\n",
    "data['Label'] = 1\n",
    "y_pred = data.iloc[:, 2:6].values\n",
    "# print(y_pred)\n",
    "y_true = data['Label'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y_pred = scaler.fit_transform(y_pred)\n",
    "\n",
    "mlp = CustomMLPRegressor(hidden_layer_sizes=(10,7,4), activation='relu', solver='adam', max_iter=10000)\n",
    "\n",
    "mlp.fit(y_pred, y_true)\n",
    "\n",
    "weights = mlp.coefs_[-1]  # Access the last layer weights\n",
    "\n",
    "abs_weights_sum = np.sum(np.abs(weights))\n",
    "\n",
    "normalized_weights = weights / abs_weights_sum\n",
    "\n",
    "print(\"Weights obtained:\\n\")\n",
    "print(\"Open AI Classifier:\", normalized_weights[0][0])\n",
    "print(\"GPT Zero:(perplexity)\", normalized_weights[1][0])\n",
    "print(\"GPT Zero:(burstiness)\", normalized_weights[2][0])\n",
    "print(\"DetectGPT:(Z score)\", normalized_weights[3][0])\n",
    "print(\"\\n\")\n",
    "\n",
    "# y_combined = np.dot(y_pred, normalized_weights)\n",
    "# print(\"Predicted labels:\\n\", y_pred)\n",
    "\n",
    "print(\"sum of weights:\\n\", \n",
    "      np.abs(normalized_weights[0][0])+np.abs(normalized_weights[1][0])+\n",
    "      np.abs(normalized_weights[2][0])+np.abs(normalized_weights[3][0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
